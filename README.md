# RESTFUL service with Lambda function (II)
#### Name: Amey Kasbe
#### UIN: 674285381
#### Email: akasbe2@uic.edu

## Project Description
The intention of the project is to analyse log data generated by [LogFileGenerator](https://github.com/ameykasbe/LogGeneratorHW3).

### Job 1
Deploy an instance of the log file generation program on EC2 and configure it to run for some period of time producing and storing log messages into log file in some S3-allocated storage.

### Job 2
To develop an algorithm of at most O(log N) complexity for locating messages from a set of N messages that contain some designated pattern-matching string that are timestamped within some delta time interval from the requested time. Create a lambda function with the algorithm that returns appropriate message with count of the log messages with 400 or 200-level HTTP client response.

### Job 3
Invoke the lambda functions by using the AWS API Gateway with the requests GET and POST and DELETE.


## Job1
### Changes in log generator
* The log generator given in the project description is - https://github.com/0x1DOCD00D/LogFileGenerator
* The log generator used to develop the solution is - https://github.com/ameykasbe/LogGeneratorHW3
* The changes are in `logback-test.xml`.
    * In the log events the time of the log event now contains date as well - "yyyy-MM-dd HH:mm:ss.SSS"
    * Changed the name of the log file to - LogFile.log
    * Log events are now appended in the same file.

### Setup

**Only those steps are mentioned that requires to select non-default values**

1. Create a EC2 instance
    1. Sign in to AWS
        1. Navigate to AWS EC2
        2. Click on Launch instance
            1. Select "Amazon Linux 2 AMI (HVM), SSD Volume Type"
            2. Download the key-pair in the process
2. Create IAM role for EC2 to give permission to access S3 buckets
    1. Navigate to AWS IAM Permissions
        1. Select roles from the left panel
        2. Click on create roles
            1. Click on EC2 in common cases
            2. Search for "AmazonS3FullAccess"
            3. Create role
3. Attach the created IAM role to the instance
    1. Navigate to EC2 instance
    2. Click on actions -> Security -> Modify IAM roles
    3. Select the newly created role
    4. Click on save
4. Login to the EC2 instance
    1. Open terminal in the local setup
    2. Copy the permission file downloaded in step 1 of this guide to root directory
    3. Give access permission to the file
       chmod 400 <.pem_file>

    4. Copy the Public DNS of the instance from AWS portal.
        1. Connect -> SSH client -> The example is the public DNS
    5. Paste and enter it into the terminal
    6. Become the root user of the EC2 instance by command <br />
       `sudo su`
5. Install Java, SBT and Git into the EC2 instance <br />
    1. Java
       `wget https://download.oracle.com/java/17/latest/jdk-17_linux-x64_bin.rpm` <br />
       `rpm -ivh jdk-17_linux-x64_bin.rpm` <br />
    2. SBT
       `curl -L https://www.scala-sbt.org/sbt-rpm.repo > sbt-rpm.repo` <br />
       `sudo mv sbt-rpm.repo /etc/yum.repos.d/` <br />
       `yum install sbt` <br />
    3. Git
       `yum install git` <br />
6. Clone the git repository
   `git clone https://github.com/ameykasbe/LogGeneratorHW3.git`

7. Compile and test run
   `sbt clean compile` <br />
   `sbt clean compile run` <br />
8. Create a S3 bucket.
9. Setup cron job
    1. Create cron script
        1. In the root directory create a cron script e.g. `cronscript.sh`
        2. Enter three commands
            1. Move to the log generator directory
            2. Generate logs
            3. Copy the log file into S3 bucket <br />
               Example - `cd /root/gRPC-REST-lambda/; sbt clean compile run && aws s3 cp /root/gRPC-REST-lambda/log/LogFile.log s3://ameykasbe-cs441-hw3-bucket;`
    2. Give access permission to the file
       `chmod +x cronscript.sh`
    3. Setup cron job
        1. Navigate to root directory
        2. Open crontab. <br />
           `crontab -e`
        3. Enter the cron job details. For example to generate logs everyday at 6 PM -
           `00 18 * * * /root/cronscript.sh`


## Job2
### [Lambda Function](https://aws.amazon.com/lambda/)
* Lambda function is a serverless compute service which can be triggered by some events.
### Algorithm
* Lambda function can be found in the directory lambda/ with name `lambda_grpc.py`
* The algorithm used is Binary Search.
    * The first log event's position becomes the high pointer
    * The last log event's position becomes the low pointer.
    * The log is divided into half (figuratively) in every iteration and is checked if the time of the middle log event (with pointer mid) falls between the required duration
    * The log is continuously divided until the any log is found with time inside the required time duration
    * From that log event, logs are traversed in both directions until the log arrives which are not in the required time duration.
    * While traversal, the log message is searched for required pattern, if it is found, a counter (initialized as 0) is increased.
    * At last the output with proper status code and message are sent as response.
* Status codes
    * `200` - Pattern present
    * `404` - Pattern not present. OR Time duration requested is not in the log file.
* Configuration parameters are passed as request parameters.

### [AWS API Gateway](https://aws.amazon.com/api-gateway/)
* AWS API Gateway is a service that makes it easier to create and maintain APIs.

### Setup AWS Lambda and AWS API Gateway

1. Setup AWS Lambda function
    1. Navigate to AWS Lambda
    2. Create Function
        1. Select from scratch
            1. Enter name
            2. Select Python as language
            3. Enter the code from lambda/lambda_grpc.py
            4. Deploy
2. Setup API gateway
    1. Navigate to AWS Lambda function
    2. Click on Add Trigger
    3. Select API Gateway
    4. Create an API
        1. Configuration
            1. API Type - REST
            2. Security - Open

* Gateway is now open to internet
* Lambda function will be triggered by HTTP GET and POST requests.
3. Create IAM role for Lambda to give permission to access S3 buckets
    1. Navigate to IAM Permissions
    2. Select roles from the left panel
    3. Click on Create role
        1. Click on Lambda in common cases
        2. Search for "AmazonS3FullAccess"
        3. Create role
4. Attach the created IAM role to the Lambda
    1. Navigate to the Lambda function
    2. Click on Configuration -> Permission -> Execution role
    3. Click on Edit under Execution roles
    4. Select the role name given above e.g. LAMBDA-S3-TEST

5. Test the lambda function with format given in lambda/ directory.


## Job3
### HTTP requests using Akka and scala.io.source
* Please navigate to the link -
  https://github.com/ameykasbe/cs441-hw3-lambda-rest-http

## gRPC

### Setup
1. Setup plugins
* Create a `plugins.sbt` file under `project` directory
* Add sbt compiler plugin <br/>
`addSbtPlugin("com.thesamet" % "sbt-protoc" % "1.0.2")` <br />
`libraryDependencies += "com.thesamet.scalapb" %% "compilerplugin" % "0.11.6"`

2. Add dependencies
* Check dependencies in the `build.sbt`.

3. Install protobuf
* For mac, <br />
`brew install protobuf`
* Check location of the protobuf compiler `protoc`. Default location is `/opt/homebrew/Cellar/protobuf/3.17.3/bin`
* Update the path in `build.sbt` and reload

4. Define message format in protobufs. Defined in `LogAnalyzerProtobuf.proto`

5. Generate Scalapb files for gRPC <br />
`sbt clean compile`

* Files are generated in `target/scala_version/main/scalapb/`
* Using these files, server and services are defined and executed.


### Configuration
* `application.conf` contains the configurations.
* Configurations are explained in the comments.

## Execution Process
### Local Environment Execution
1. Clone this repository
2. In terminal, navigate to root path of the project
3. Execute - <br />
   `sbt clean compile run`
4. As there is no main class defined the terminal will prompt a message - Multiple main classes detected. Select one to run:
   [1] LogAnalyzerClient
   [2] LogAnalyzerServer
5. Select 2 AkkaHttp to initiate server
    1. When you receive the message that server has started proceed ahead
6. Repeat and select 1 to activate client and request. 

### Unit testing procedure
#### Using IntelliJ Idea
1. Clone this repository
2. Import the project in IntelliJ Idea
3. Run the `GrpcTestSuite` class from `GrpcTest` package under test/scala.

#### By SBT test command
1. Clone this repository
2. In terminal, navigate to root path
3. Execute - <br />
   `sbt clean compile test`


### References
1. Dr. Grechanik, Mark, (2020) Cloud Computing: Theory and Practice.
2. [Scala Days Conferences](https://www.youtube.com/watch?v=778znDnjROg)

  